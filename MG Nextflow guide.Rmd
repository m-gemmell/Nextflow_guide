---
title: "MG Nextflow Guide"
author: "Matthew R. Gemmell"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
favicon: figures/nextflow_favicon.png
description: NEOF book for the Introduction to command line bioinformatics workshop
cover-image: "figures/nextflow_favicon.png"
---
```{r include=FALSE, cache=FALSE}
library(webexercises)
```

```{r cite-packages, include = FALSE}
# automatically create a bib database for R packages
# add any packages you want to cite here
knitr::write_bib(c(
  .packages(), 'bookdown', 'webexercises', 'qmethod'
), 'packages.bib')
```

```{r, echo=FALSE}
#Change colour, border, and text of code chunks
#Check style.css for .Rchunk
#https://stackoverflow.com/questions/65627531/change-r-chunk-background-color-in-bookdown-gitbook
#https://bookdown.org/yihui/rmarkdown-cookbook/chunk-styling.html
knitr::opts_chunk$set(class.source="Rchunk") 
```

# Intro

<center>
![](figures/nextflow.png){style="width:200px; background: black; border-radius:15px; border: black 10px solid"}
</center>

This is a quick and dirty guide on how I use Nextflow.
It is primarily for my use but if someone else finds it I hope it will also proof useful. 
Additionally, this was made with R Bookdown.

## Running nextflow

To run nextflow move to the directory with the `.nf` file you want to run and run:

```{bash, eval=FALSE }
nextflow run main.nf -resume
```

`-resume` is useful to keep so it does not rerun any processes that have already run. However, it can be useful to remove it if you want to test the workflow from the start.

## Contents

The chapters in this bookdown include:

[Concepts](#concepts): 

## Links

Useful links include:

- [nextflow.io](https://www.nextflow.io/): The main website for Nextflow
- [training.nextflow.io](https://training.nextflow.io/): Website with tutorials for Nextflow

<!--chapter:end:index.Rmd-->

```{r include=FALSE, cache=FALSE}
library(webexercises)
```
# Concepts

## Workflow

Nextflow is used to create workflows.
A workflow consists of multiple __processes__.
Each __process__ has __input__ and __output__.

A __process__ will not start until it has all the __input__ it requires.
This is important as a process will therefore wait till other __processes__ are finished if the __output__ of other __processes__ are the __input__ for the __process__.

In the nextflow script each process is created in the main script body (or in a separate `main.nf` file).

These __processes__ are then called in the __workflow__.

Example `main.nf`

```{bash, eval=FALSE }
#!/usr/bin/env nextflow
/*
  *Params
*/
params.dir = "."
params.input1 = "input.txt"
/*
  * Processes
*/
process STEP1 {
  input:
    path(input_data)
  output:
    path("output1.txt")
}
process STEP2 {
  input:
    path(input_data)
  output:
      path("output2.txt")
}
/*
  * workflow
*/
workflow {
  /// Set input param to channel
  input1_ch=file(params.input1)
  /// Process 1
  step1(input1_ch)
  /// Process 2
  step2(step1.out)
}
``` 

## Variables

Nextflow __Variables__ are used across a nextflow script.
Within the `script` section of a `process` block they are denoted by `$` (e.g. `$sample_id`).

__Variable__ names cannot start with numbers.

Bash __variables__ within a `script` section must be denoted by a `\` (e.g. `\$var`).

## Channels, tuples, & lists {#channels_tuples_lists_concept}

__Channels__, __tuples__, and __lists__ are objects that contain multiple objects but work in different ways.

### Channels

__Channels__ are specified and used in the __workflow__ section. 
A __Channel__ contains a number of __values__.
Each __value__ passes through a process separately, this is carried out via parallelisation.

Example:

- A __Channel__ called `integers_ch` contains the 10 values 1,2,3,4,5,6,7,8, & 9.
- A process called `multiple_by_10` multiples input by 10.
- If the __Channel__ `integers_ch` was used as the input for the __process__ `multiple_by_10` the output woudl be a __Channel__ of 10 values containing the __values__ 10,20,30,40,50,60,70,80, & 90.

### Tuples & Lists

__Tuples__ & __Lists__ are used within __process__ blocks.
There are many ways to create and manipulate them within and without __process blocks__.

Confusingly __Tuples__ & __Lists__ are both structured as `[value_1,value_2,value_3]`.

#### Tuples

__Tuples__ contain multiple values with each value assigned as a separate variable in a process.
This allows you to input/output data which should be grouped together.
The below example shows how to group sample ids with their paired fastq reads.

```{bash, eval=FALSE }
process step1 {
  input:
    tuple val(sample_id), path(r1), path(r2)
  output:
    tuple val(sample_id), path("${sample_id}_r1_trimmed.fastq"), path("${sample_id}_r2_trimmed.fastq")
  script:
  """
  trim -i1 $r1 -i2 $r2 \
  -o1 ${sample_id}_r1_trimmed.fastq -o2 ${sample_id}_r2_trimmed.fastq
  """
}
```

This is important as multiple __Channels__ with multiple __values__ are not ordered relative to each other.

#### Lists

When a __List__ is used within a __script__ block all the __values__ will be used together with a space (` `) between each value.

Example:

The __Channel__ `r1_fastqs_ch` contains the __List__ `["S1_R1.fastq","S2_R1.fastq","S3_R1.fastq"]`

The below truncated nextflow script to run fastqc....

```{bash, eval=FALSE }
process r1_fastqc {
  input:
    path(r1s)
  output:
    .....
  script:
  """
  fastqc -o fastqc_output \
  $r1s
  """
}
workflow {
  r1_fastqc(r1_fastqs_ch)
}
```

Would be run as:

```{bash, eval=FALSE }
fastqc -o fastqc_output \
S1_R1.fastq S2_R1.fastq S3_R1.fastq
```

#### Combinations

Of course you can have a __Channel__ that can contain multiple __Tuples__ and/or __Lists__. Additionally __Tuples__ can contain __Lists__.

<!--chapter:end:01-Concepts.Rmd-->

```{r include=FALSE, cache=FALSE}
library(webexercises)
```
# (PART\*) Components {-}
# Basic layout

When using Nextflow each project should be within its own directory.

Within the main directory of the project you will have all the Nextflow files you need.

There are many Nextflow files you can have but the only essential file is a main `.nf` file which many people like to name as `main.nf`.

This chapter will go over a basic `main.nf` file and its different parts.

## `main.nf`

The `main.nf` is a plain text file that contains Nextflow code.
Below is an example:

```{bash, eval=FALSE }
#!/usr/bin/env nextflow
/*
  *Params
*/
params.dir = "."
params.outdir = "./results"
params.input_file = "/path/to/data/input.txt"

/*
  * Processes
*/
process STEP1 {
  publishDir:
    params.outdir, mode: 'copy'
  input:
    path(input_data)
  output:
    path("output1.txt")
  script:
  """
  create_output -i $input_data -o output1.txt
  """
}

process STEP2 {
  publishDir:
    params.outdir, mode: 'copy'
  input:
    path(input_data)
  output:
    path("output2.txt")
  script:
  """
  create_output_2 -i $input_data -o output2.txt
  """
}

/*
  * workflow
*/
workflow {
  /// Set input params to channels
  input_ch=file(params.input_file)
  /// Process 1
  STEP1(input_ch)
  /// Process 2
  STEP2(step1.out)
}
``` 

## Params

The first section is the various initial parameters. This is useful for specifying input information for the workflow including:

- Input & output directories
- Input data
- Metadata information
- Reference files

Any time they are used anywhere within the script they must include `params.` as a prefix.

## Processes

__Processes__ are how the tasks of a workflow are specified and have many parts.

[__Process reference__](https://www.nextflow.io/docs/latest/reference/process.html#process-reference)

### Initialistion

A __process__ is defined by `process NAME` and the __process__ body is cotnained with `{}`.

The __Process__ name is arbitrary and decided by the workflow designer.
However some suggestions are:

- Do not start the name with numbers.
- Capitalise all letters used, this is normal convention and makes it easier to see what are __processes__ in your __workflow__ block.
- Separate words with `_`.
- Ensure all process names are unique and somewhat descriptive within your workflow.

### publishDir {#basic_publishdir}

All files created by the __Process__ will be contained with the workflow's `work` directory (more info later).
All files specified in the `output` section will be stored in the directory specified by `publishDir`, if the directory does not exist Nextflow will create it.
The output data is stored in the specified based on the `mode:`.

The modes are:

- `copy`: Copies the output files into the publish directory.
- `copyNoFollow`: Copies the output files into the publish directory without following symlinks ie. copies the links themselves.
- `link`: Creates a hard link in the publish directory for each output file.
- `move`: Moves the output files into the publish directory. Note: this is only supposed to be used for a terminal process i.e. a process whose output is not consumed by any other downstream process.
- `rellink`: Creates a relative symbolic link in the publish directory for each output file.
- `symlink`: Creates an absolute symbolic link in the publish directory for each output file (default).

More info on [`publishDir`](https://www.nextflow.io/docs/latest/reference/process.html#publishdir)

### input

These specify the input names and type.
Input names are arbitrary but should follow variables rules.

The 2 basic input types are:

- `path`: Specifies paths, generally poiting to files.
- `val`: Specifies values, these are generally text or numbers such as sample ids.

### output

These specify the output.

If the output is a file it needs to be the actual name of the output file in quotes (double quotes are normally preferred).

### script

The script block, denoted by flanking triple double quotes (`"""`), contains the code the __process__ will carry out.
By default this will be `bash` code.

Input variables can be denoted by a `$` and output variables are written like normal script but can also use input variables such as sample ids.

## workflow
The workflow section is where all the other sections come together so the workflow knows how to utilise them.

<!--chapter:end:02-Basic_layout.Rmd-->

```{r include=FALSE, cache=FALSE}
library(webexercises)
```
# Channels, tuples, & lists

This section shows how to create, edit, and manipulate channels, tuples, and lists.

For an explanation on these concepts please see [chapter 2](#channels_tuples_lists_concept)

## Channels

For more info on channels including channel operators please see the [nextflow docs](https://www.nextflow.io/docs/latest/channel.html)

### Creating channels {#creating_channels}

There are many ways to create various types of channels.

#### Text

```{bash, eval=FALSE }
workflow {
  hello_ch = Channel.of("Hello","Bonjour")
}
```

#### Data file

```{bash, eval=FALSE }
data_ch = Channel.fromPath(params.input_file)
```

or

```{bash, eval=FALSE }
workflow {
  data_ch = file(params.input_file)
}
```

#### CSV of files

```{bash, eval=FALSE }
workflow {
  files_ch = Channel.fromPath(params.input_file)
	  .splitCsv()
	  .flatten()
}
```

#### FOFN

A file containing file names sperated by lines

```{bash, eval=FALSE }
workflow {
  files_ch = Channel.fromPath(params.file).splitText()
}
```

#### Paired files

Useful for paired illumina fastq files.

```{bash, eval=FALSE }
params.reads = "/path_to_fastq_dir/*_R{1,2}.fastq"
workflow {
  Channel
    .fromFilePairs(params.reads, checkIfExists: true)
    .set {read_pairs_ch}
}
```

This creates a channel of multiple values based on the number of paired files. Each channel value has contains a tuple of 2 values:

- First is a text value of the file prefix. The file prefix is the text that the `*` reprsents in the params.reads path.
- The second is a list with the first value is the R1 file and the second value is the R2 file.

For example if you had the files:

- S1_R1.fastq
- S1_R2.fastq

The tuple would be:

`["S1",["S1_R1.fastq","S1_R2.fastq]]`

You could then use this channel like so:

```{bash, eval=FALSE }
params.reads = "/path_to_fastq_dir/*_R{1,2}.fastq"
process RUN {
  input:
    tuple val(sample_id), path(reads)
  script:
  """
  command -s ${sample_id} -r1 ${reads[0]} -r2 ${reads[1]}
  """
}
workflow {
  Channel
    .fromFilePairs(params.reads, checkIfExists: true)
    .set {read_pairs_ch}
  RUN(read_pairs_ch)
}
```

__Note:__ Indexing a list starts at 0 (`-r1 ${reads[0]}`)

### Extracting channels from process

```{bash, eval=FALSE }
process RUN {
  input:
    path(R1)
  output:
    path("R1_trimmed.fastq"), emit: R1_trimmed
    path("R1_trimmed.stats.txt"), emit: R1_trimmed_stats
}
workflow {
  /// Input reads from params
  r1_ch = file(params.r1_fastq)
  /// Run process
  RUN(r1_ch)
  /// Extract channels from run process
  r1_trimmed_ch = RUN.out.R1_trimmed
  r1_trimmed_stats_ch = RUN.out.R1_trimmed_stats
}
```

__Note:__ You only need to extract channels form a process if they are going to be used in another process.

### View

You can view a channel's content and structure by including `.view` and running the workflow (`nextflow run main.nf`).

```{bash, eval=FALSE }
workflow {
  /// Input reads from params
  r1_ch = file(params.r1_fastq).view
}
```


## Tuples

### Input & Output tuples

```{bash, eval=FALSE }
process RUN {
input:
  tuple val(id), path(R1), path(R2)
output:
	tuple val(id), path(“R1_trim.fastq”), file(“R2_trim.fastq”)
}
```

### Transpose

If you have the below channel of 3 values each containing a tuple of 2....

`[[“s1”,”s1.txt”],[“s2”,”s2.txt”],[“s3”,”s3.txt”]]`

and want to change it to the below channel of 1 value containing a tuple of 2, each tuple containing a list of 3....

`[[“s1”,”s2”,”s3”],[“s1.txt”,”s2.txt”,”s3.txt”]]`

You can run the following

```{bash, eval=FALSE }
workflow {
  transposed_ch = id_file_pair_ch.collect(flat:false)
                                  .map{ it.transpose()}
}
```

This can the be called as input to a process like so:

```{bash, eval=FALSE }
process RUN {
  input:
    tuple val(sample_ids_list), path(files_list)
}
```


## List

### Channel values to one list

You can collect all the values of a channel into one list.

```{bash, eval=FALSE }
workflow {
  all_R1_fastq_ch = FASTQC.out.htmlR.collect()
  all_R2_fastq_ch = FASTQC.out.htmlL.collect()
}
```

This ends up with lists like
[“S1_R1.fastqc.html”, “S2_R1.fastqc.html”]

### Parameter in front of each list value

Some programs accept a list of values separated by spaces.
However, some may require a parameter flag in front of each path/file.
This can be carried out by setting a definition in the process block.

```{bash, eval=FALSE }
process RUN {
  input:
  	path(all_R1_fastq)
  
  script:
  def R1_fastqc_lines = all_R1_fastq.collect { 
    fastqc -> “-F ${fastqc}”
  }.join(‘ ‘)
  """
  multiqc -o multiqc $R1_fastqc_lines
  """
}
```

That then becomes 
`“-F S1_R1.fastqc.html -F S2_R1.fastqc.html”`




<!--chapter:end:03-Channels_tuples_lists.Rmd-->

```{r include=FALSE, cache=FALSE}
library(webexercises)
```
# Processes

This section includes some extra features related to __processes__.

A full link of process info can be seen in the [nextflow docs](https://www.nextflow.io/docs/latest/reference/process.html)

## Directives {#process_directives}

You can include directives in each process. These can be used to specify the execution requirements of a process.

[Link to full list of process directives](https://www.nextflow.io/docs/latest/reference/process.html#directives)

### cpus

One example is that you can use `cpus` to specify the number of cpus to be used by a process.

```{bash, eval=FALSE }
process RUN {
  cpus 2
}
```

### publishDir

The basics of [pblishDir](#basic_publishdir)

My preferred method is to assign an overall output/results directory as a params.

```{bash, eval=FALSE }
params.outdir="./results"

process RUN {
  publishDir params.outdir, mode: 'copy'
}
```

You can also set a subdiretcory of the output directory in the process.

```{bash, eval=FALSE }
params.outdir="./results"

process RUN {
  publishDir {
    "${params.outdir}/stage_1"
    }, mode: 'copy'
}
```

You can even do this with input variables.

```{bash, eval=FALSE }
params.outdir="./results"
input:
  val sample_id
process RUN {
  publishDir {
    "${params.outdir}/stage_1/${sample_id}"
    }, mode: 'copy'
}
```

### conda

A conda environment can be specified.

Further info on using [conda environments](https://www.nextflow.io/docs/latest/conda.html#conda-page)

#### Local environment

You can specify a conda environment you have locally created.

```{bash, eval=FALSE }
process RUN {
  conda "/home/minforge3/envs/run_env"
}
```

If using a locally installed env it is best to specify it as a params to make it quicker to add/edit for multiple processes.

```{bash, eval=FALSE }
params.conda_env="/home/minforge3/envs/run_env"

process RUN {
  conda params.conda_env
}
```

#### URI based environment

You can have nextflow install a conda packages for specific process.

```{bash, eval=FALSE }
process RUN {
  conda "bioconda::samtools=1.20"
}
```

You can find what packages can be downloaded this way through [sequera containers](https://seqera.io/containers/).

For an easy example search for `bioconda::samtools` on the above link.

#### Setting workflow to conda usage

When you want to use the specified conda environments in a workflow you must either:

Include `-with-conda`/`-use-conda` in the `nextflow run` command

or:

Better yet add `conda.enabled=true` to your `nextflow.config` file

## Script

### Variables

Within the script nextflow variables are called as `${samples_id}`.

Bash variables are called as `\${sample_id}`

### Other languages

Other languages can be used within the nextflow script section.

For example python:

```{bash, eval=FALSE }
script
"""
#!/usr/bin/env python
"""
```

## Modules

The primary `main.nf` can become quite large by having a lot of __processes__.
To counteract this each process can be stored in a separate `main.nf` file.

The recommendation is to store them in a directory called `modules/local` within the main workflow directory.
Then each process would be within a `main.nf` file within various directories.

A module `main.nf` would be as so:

```{bash, eval=FALSE }
#!/usr/bin/env nextflow
process RUN {
  / process contents
}
```

Modules are imported as a process as so:

```{bash, eval=FALSE }
/*
 * Processes
*/
include { FASTQC_RAW } from './modules/local/run/main.nf'
```

It is common to have subdirectories within `modules/local` grouped by tools. For example if you were performing 16S analysis with qiime2 you may have some of the following modules:

```{bash, eval=FALSE }
include { IMPORT } from './modules/local/qiime2/import_data/main.nf'
include { CUTADAPT } from './modules/local/qiime2/cutadpat/main.nf'
include { DADA2 } from './modules/local/qiime2/dada2/main.nf'
```

<!--chapter:end:04-Processes.Rmd-->

```{r include=FALSE, cache=FALSE}
library(webexercises)
```
# Workflow

This section includes some extra features related to __workflows__.

A full link of workflow info can be seen in the [nextflow docs](https://www.nextflow.io/docs/latest/workflow.html)


<!--chapter:end:05-Workflow.Rmd-->

```{r include=FALSE, cache=FALSE}
library(webexercises)
```
# Params and config

This section includes some extra features related to __params__ and the __config__ file.

A full link of config info can be seen in the [nextflow docs](https://www.nextflow.io/docs/latest/config.html)

## Params

Workflow parameters can be set at the top of the primary `main.nf` file.

```{bash, eval=FALSE }
/*
 * Params
*/
params.dir = "."
params.outdir = "./results"
params.sample_name = "Sample_1"
params.fastqs = "/home/data/project123/raw_fastqs"
params.database = "/home/dbs/db123/db123.fastq"
params.conda_env = "/home/conda/envs/example_env"
params.rscript = "./rscripts/summary.rscript"
```

### Params to channel

When using params containing a file in the workflow it is common practice to convert them to a channel.

```{bash, eval=FALSE }
workflow {
  database_ch = file(params.database)
}
```

More info/examples in [Creating channels](#creating_channels)

## Config

You can contain a lot of info in a config file.

Your primary config file should be in the same directory as your primary `main.nf` file and should be named `nextflow.config`.
This will cause it to be used when you `nextflow run main.nf -resume`.

[Link to nextflow configuration doc](https://www.nextflow.io/docs/latest/config.html)

### Process directives

For [Process directives](#process_directives) you want to be the default of all processes in your workflow you can include them in your `nextflow.config`.

```{bash, eval=FALSE }
process {
  memory = 100.00GB
  cups = 8
  time = 30.d
}
```

### Params in config

You can include your `params` in your config file.

```{bash, eval=FALSE }
params {
  dir = "."
  outdir = "./results"
  sample_name = "Sample_1"
  fastqs = "/home/data/project123/raw_fastqs"
  database = "/home/dbs/db123/db123.fastq"
  conda_env = "/home/conda/envs/example_env"
  rscript = "./rscripts/summary.rscript"
}
```

Ensure you still use `params.` as a prefix in the various `.nf` files.

### Conda

Rather than needing to include `-use-conda` in your command line you can add the below to your `nextflow.config` to make your workflow use conda.

```{bash, eval=FALSE }
conda.enabled = true
```

### Container program

There are many different container programs. Below are what you can add to the `nextflow.config` to get the different ones to work.

This includes setting the path to the image (if using one) and enabling the specific container program.

```{bash, eval=FALSE }
/// Docker
process.container = 'nextflow/examples:latest'
docker.enabled = true
```

```{bash, eval=FALSE }
/// Singularity
process.container = '/path/to/singularity.img'
singularity.enabled = true
```

If using one of them ensure you specify the containers in the variosu processes.

[Link to container nextflow docs](https://www.nextflow.io/docs/latest/container.html)

### Config profiles


You can have multiple profiles in a `nextflow.config` file.

To use a profile you would run:

```{bash, eval=FALSE }
nextflow run main.nf -profile laptop
```

To make different profiles you could include the below in your `nextflow.config`.

```{bash, eval=FALSE }
profiles {
    laptop {
        process.executor = 'local'
        docker.enabled = true
    }
    hpc {
        process.executor = 'slurm'
        conda.enabled = true
        process.resourceLimits = [
            memory: 750.GB,
            cpus: 200,
            time: 30.d
        ]
    }
}
```



<!--chapter:end:06-Params_and_config.Rmd-->

```{r include=FALSE, cache=FALSE}
library(webexercises)
```
# (PART\*) Example workflows {-}
# Illumina QC example

This chapter contains an example nextflow wrokflow for a simple QC of paired Illumina data.

In this example `./` represents the main project/workflow directory.

It will use a locally installed conda env.

## main.nf

The main nextflow file.

Path: `./main.nf`

```{bash, eval=FALSE }
#!/usr/bin/env nextflow

/*
 * Workflow for basic Paired end Illumina QC
*/

/ Params in nextflow.config

/Text that will be printed at start of work flow to terminal
log.info """\
  ILLUMINA QC
  ===========
  raw reads: ${params.reads}
  out_dir: ${params.outdir}
  """
  .stripIndent()
  
/*
* Processes
* All within module files
*/

include { FASTQC_RAW } from './modules/local/fastqc/raw/main.nf'
include { MULTIQC_RAW } from './modules/local/multiqc/raw/main.nf' 
include { TRIMMOMATIC } from './modules/local/trimmomatic/main.nf' 
include { FASTQC_TRIM } from './modules/local/fastqc/trim/main.nf'
include { MULTIQC_TRIM } from './modules/local/multiqc/trim/main.nf'

/*
* Workflow
*/
  /// Read in fastq data into channel
  Channel
    .fromFilePairs(params.reads, checkIfExists: true)
    .set {read_pairs_ch}
  /// raw fastqc
  FASTQC_RAW(read_pairs_ch)
  /// raw fastqc zip output
  raw_fastqc_zip_ch=FASTQC_RAW.out.zip
  /// Transform so channel of 1 value which is a tuple
  /// all R1 in one list and all R2 in one list
  raw_fastqc_zip_split_by_r1r2_ch = raw_fastqc_zip_ch.collect(flat:false)
                                                      .map{it.transpose()}
  /// raw multiqc of fastqc
  MULTIQC_RAW(raw_fastqc_zip_split_by_r1r2_ch)
  /// trimmomatic
  TRIMMOMATIC(read_pairs_ch)
  /// trimmomatic fastqc
  FASTQC_TRIM(TRIMMOMATIC.out)
  /// trim fastqc zip output
  trim_fastqc_zip_ch=FASTQC_RAW.out.zip
  /// Transform so channel of 1 value which is a tuple
  /// all R1 in one list and all R2 in one list
  trim_fastqc_zip_split_by_r1r2_ch = trim_fastqc_zip_ch.collect(flat:false)
                                                      .map{it.transpose()}
  /// trimmomatic_multiqc
  MULTIQC_TRIM(trim_fastqc_zip_split_by_r1r2_ch)
```

## nextflow.config

The main confg file.

Path: `./nextflow.config`

```{bash, eval=FALSE }
conda.enabled = true

params {
  dir = "."
  outdir = "./results"
  reads = "/home/data/project123/fastqs/*_R{1,2}.fastq"
  conda_env = "/home/conda/envs/example_env"
}

process {
  memory = 100.00GB
  cups = 8
  time = 30.d
}
```

## Modules

The processes are each in their own separate files/modules.

### FASTQC_RAW

Path: `./modules/local/fastqc/raw/main.nf`

__Note:__ how in this case the `publishDir` directive is used to create the output directory where the fastqc files will be located.

```{bash, eval=FALSE }
#!/usr/bin/env nextflow
process FASTQC_RAW {
  conda params.conda_env
  
  publishDir {
    "${params.outdir}/raw_fastqc_output"
    }, mode:'copy'
  
  input:
    tuple val(sample_id), path(reads)
    
  output:
    tuple path("./${sample_id}_R1_fastqc.html"),
      path("./${sample_id}_R2_fastqc.html"), emit: html
    tuple path("./${sample_id}_R1_fastqc.zip"),
      path("./${sample_id}_R2_fastqc.zip"), emit: zip
  script:
  """
  fastqc -o ./ ${reads[0]} ${reads[1]}
  """
}
```

### MULTIQC_RAW

Path: `./modules/local/multiqc/raw/main.nf`

Output is simple as we don't need to use it in the workflow as it is part of the final output.

```{bash, eval=FALSE }
#!/usr/bin/env nextflow
process MULTIQC_RAW {
  conda params.conda_env
  
  publishDir {
    "${params.outdir}/raw_multiqc_output"
    }, mode:'copy'
  
  input:
    tuple path(r1_zip), path(r2_zip)
    
  output:
    path "./r{1,2}/multiqc_report.html"
  
  script:
  """
  #R1
  mkdir r1
  multiqc -o r1 $r1_zip
  #R2
  mkdir r2
  multiqc -o r2 $r2_zip
  """
}
```

### TRIMMOMATIC

Path: `./modules/local/trimmomatic/main.nf`

Trimmomatic creates:

- __P files:__ Paired reads that retained both pairs.
- __U files:__ Reads that are unpaired as their paired read was removed by the filtering. These are generally not used for most processes.

```{bash, eval=FALSE }
#!/usr/bin/env nextflow
process TRIMMOMATIC {
  conda params.conda_env
  
  publishDir params.outdir, mode:'copy'
  
  input:
    tuple path(sample_id), path(reads)
    
  output:
    tuple val(sample_id), path("${sample_id}_{1,2}P.fastq"), path("${sample_id}_{1,2}U.fastq")
  
  script:
  """
  trimmomatic PE -phred33 \
  ${reads[0]} ${reads[1]} \
  -baseout ${sample_id}.fastq \
  SLIDINGWINDOW:4:20 MINLEN:50
  """
}
```

### FASTQC_TRIM

Path: `./modules/local/fastqc/trim/main.nf`

Only using fastqc on the __P__ files as we are not interested in the __U__ files.

```{bash, eval=FALSE }
#!/usr/bin/env nextflow
process FASTQC_TRIM {
  conda params.conda_env
  
  publishDir {
    "${params.outdir}/trim_fastqc_output"
    }, mode:'copy'
  
  input:
    tuple val(sample_id), path(trim_p_reads), path(trim_u_reads)
    
  output:
    tuple path("./${sample_id}_1P_fastqc.html"),
      path("./${sample_id}_2P_fastqc.html"), emit: html
    tuple path("./${sample_id}_2P_fastqc.zip"),
      path("./${sample_id}_2P_fastqc.zip"), emit: html
  script:
  """
  fastqc -o ./ ${trim_p_reads[0]} ${trim_p_reads[1]}
  """
}
```

### MULTIQC_TRIM

Path: `./modules/local/multiqc/trim/main.nf`

```{bash, eval=FALSE }
#!/usr/bin/env nextflow
process MULTIQC_TRIM {
  conda params.conda_env
  
  publishDir {
    "${params.outdir}/trim_multiqc_output"
    }, mode:'copy'
  
  input:
    tuple path(r1_zip), path(r2_zip)
    
  output:
    path "./r{1,2}/multiqc_report.html"
  
  script:
  """
  #R1
  mkdir r1
  multiqc -o r1 $r1_zip
  #R2
  mkdir r2
  multiqc -o r2 $r2_zip
  """
}
```

<!--chapter:end:07-Illumina_QC_example.Rmd-->

